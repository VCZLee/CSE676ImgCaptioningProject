{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import nltk\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import decode_predictions, preprocess_input\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "assert tf.__version__ == \"2.0.0\"\n",
    "\n",
    "PATH = \"/media/jintoboy/Main Storage/Image Captioning/\"\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    if height != width:\n",
    "        if height < width:\n",
    "            offset = int((width - height) / 2)\n",
    "            img = tf.image.crop_to_bounding_box(img, 0, offset, height, height)\n",
    "        else:\n",
    "            offset = int((height - width) / 2)\n",
    "            img = tf.image.crop_to_bounding_box(img, offset, 0, width, width)\n",
    "    img = tf.image.resize(img, (224, 224))\n",
    "    return preprocess_input(img), image_path\n",
    "\n",
    "\n",
    "def img_ids_and_captions_from_json(PATH):\n",
    "    with open(PATH + \"annotations/captions_train2017.json\", \"r\") as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    img_locations = []\n",
    "    captions = []\n",
    "    for annotation in annotations[\"annotations\"]:\n",
    "        img_locations.append(\"{}train2017/{:012d}.jpg\".format(PATH, annotation[\"image_id\"]))\n",
    "        captions.append(annotation[\"caption\"])\n",
    "\n",
    "    return img_locations, captions\n",
    "    # partially adapted from https://www.tensorflow.org/tutorials/text/image_captioning\n",
    "\n",
    "\n",
    "def cache_vgg_features(img_ids):\n",
    "    unique_ids = sorted(set(img_ids))\n",
    "    images = tf.data.Dataset.from_tensor_slices(unique_ids)\n",
    "    images = images.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "    extract_vgg_features_model = VGG16(include_top=False)\n",
    "\n",
    "    for img, path in tqdm(images):\n",
    "        batch_features = extract_vgg_features_model(img)\n",
    "        batch_features = tf.reshape(batch_features, (batch_features.shape[0],\n",
    "                                                     -1, batch_features.shape[3]))\n",
    "        for bf, p in zip(batch_features, path):\n",
    "            path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "            np.save(path_of_feature, bf.numpy())\n",
    "    # adapted from https://www.tensorflow.org/tutorials/text/image_captioning\n",
    "\n",
    "\n",
    "def get_glove_embeddings(dimensions):\n",
    "    # 400000 unique tokens in vocabulary\n",
    "    embeddings_index = {}\n",
    "    with open(os.path.join(\"{}glove_embeddings\".format(PATH), \"glove.6B.{}d.txt\".format(dimensions))) as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs + \" 0 0 0\", 'f', sep=' ')  # add 3 dimensions for <start>, <end>, and <pad>\n",
    "            embeddings_index[word] = coefs\n",
    "    # partially adapted from https://keras.io/examples/pretrained_word_embeddings/\n",
    "    \n",
    "    unk_vector = np.zeros((dimensions + 3,))\n",
    "    i = 0\n",
    "    for word in embeddings_index:\n",
    "        unk_vector += embeddings_index[word]\n",
    "        i += 1\n",
    "    unk_vector /= i\n",
    "    embeddings_index[\"<unk>\"] = unk_vector\n",
    "    embeddings_index[\"<no_glove>\"] = unk_vector\n",
    "\n",
    "    embeddings_index[\"<start>\"] = np.zeros((dimensions + 3,))\n",
    "    embeddings_index[\"<start>\"][dimensions] = 1\n",
    "\n",
    "    embeddings_index[\"<end>\"] = np.zeros((dimensions + 3,))\n",
    "    embeddings_index[\"<end>\"][dimensions + 1] = 1\n",
    "\n",
    "    embeddings_index[\"<pad>\"] = np.zeros((dimensions + 3,))\n",
    "    embeddings_index[\"<pad>\"][dimensions + 2] = 1\n",
    "\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def get_glove_embedding_matrix(processed_glove_indices, tokenizer, num_words, raw_embedding_dims):\n",
    "    embedding_matrix = np.zeros((num_words, raw_embedding_dims + 3))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i >= num_words:\n",
    "            continue\n",
    "        embedding_matrix[i] = processed_glove_indices.get(word)\n",
    "    # partially adapted from https://keras.io/examples/pretrained_word_embeddings/\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def save_preprocessed_captions(captions, glove_embedding_index):\n",
    "    glove_words = set(glove_embedding_index.keys())\n",
    "    for i in range(len(captions)):\n",
    "        tokens = [token.lower() if token.lower() in glove_words\n",
    "                  else \"<no_glove>\"\n",
    "                  for token in nltk.tokenize.word_tokenize(captions[i])]\n",
    "        captions[i] = \"<start> {} <end>\".format(\" \".join(tokens))\n",
    "    captions_file = open(\"{}annotations/tokenized_captions.txt\".format(PATH), \"w\")\n",
    "    for caption in captions:\n",
    "        print(caption, file=captions_file)\n",
    "    captions_file.close()\n",
    "\n",
    "\n",
    "def retrieve_tokenized_captions_from_file():\n",
    "    file = open(\"{}annotations/tokenized_captions.txt\".format(PATH), \"r\")\n",
    "    lines = [line.rstrip('\\n') for line in file]\n",
    "    file.close()\n",
    "    return lines\n",
    "\n",
    "\n",
    "def save_class_labels_from_capped_sequences(capped_seqs, stop_word_tokenizer_indices):\n",
    "    train_classes = [list(set([token for token in sequence if token not in stop_word_tokenizer_indices]))\n",
    "                     for sequence in capped_seqs]\n",
    "    labels_file = open(\"{}annotations/class_labels.txt\".format(PATH), \"w\")\n",
    "    for labels in train_classes:\n",
    "        print(\" \".join(map(str, labels)), file=labels_file)\n",
    "    labels_file.close()\n",
    "\n",
    "\n",
    "def retrieve_class_labels_from_capped_sequences():\n",
    "    file = open(\"{}annotations/class_labels.txt\".format(PATH), \"r\")\n",
    "    lines = [np.fromstring(line.rstrip('\\n'), int, sep=' ') for line in file]\n",
    "    file.close()\n",
    "    return lines\n",
    "\n",
    "\n",
    "def labels_to_one_hot(labels, num_labels):\n",
    "    return tf.reduce_sum(tf.one_hot(labels, depth=num_labels), axis=0)\n",
    "\n",
    "\n",
    "def one_hot_to_labels(one_hot):\n",
    "    where = tf.not_equal(one_hot, 0)\n",
    "    indices = tf.where(where)\n",
    "    value_rowids = indices[:, 0]\n",
    "    values = indices[:, 1]\n",
    "    ragged_tensor = tf.RaggedTensor.from_value_rowids(values, value_rowids)\n",
    "    return [tensor.numpy() for tensor in list(ragged_tensor)]\n",
    "\n",
    "\n",
    "def img_classifier_func(img_name, labels):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    one_hot = labels_to_one_hot(labels, num_tokens)\n",
    "    return img_tensor, one_hot\n",
    "\n",
    "\n",
    "def create_partial_image_classifier(num_tokens):\n",
    "# VGG output is 7x7x512, but the cached files are 49 x 512\n",
    "    vgg_input = tf.keras.layers.Input(shape=(49, 512))\n",
    "    flattened = tf.keras.layers.Flatten()(vgg_input)\n",
    "    dropout = tf.keras.layers.Dropout(0.5)(flattened)\n",
    "    dense_output = tf.keras.layers.Dense(num_tokens, activation=\"sigmoid\")(dropout)\n",
    "\n",
    "    model = tf.keras.models.Model([vgg_input], dense_output)\n",
    "    return model\n",
    "# model architecture inspired by https://arxiv.org/pdf/1511.05284.pdf, https://arxiv.org/pdf/1606.07770.pdf\n",
    "\n",
    "\n",
    "\n",
    "def compile_classifier_only_model(classifier_model):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "    classifier_model.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
    "    return classifier_model\n",
    "\n",
    "\n",
    "def train_image_classifier(image_classifier_model, dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for imgs, labels in dataset:\n",
    "            image_classifier_model.train_on_batch([imgs], labels)\n",
    "\n",
    "\n",
    "def create_partial_lstm_language_model(num_tokens, max_sequence_length, embedding_dims, embedding_matrix):\n",
    "    embed_matrix = tf.convert_to_tensor(embedding_matrix, dtype=tf.float32)\n",
    "\n",
    "    sequence_input = tf.keras.layers.Input(shape=(None,))\n",
    "    embedded_sequences = tf.keras.layers.Embedding(input_dim=num_tokens,\n",
    "                                                   output_dim=embedding_dims + 3,\n",
    "                                                   embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                                                   trainable=False)(sequence_input)\n",
    "\n",
    "    labels_input = tf.keras.layers.Input(shape=(None, embedding_dims + 3, ))\n",
    "\n",
    "\n",
    "    element_wise_sum = tf.keras.layers.Add()([embedded_sequences, labels_input])\n",
    "\n",
    "    lstm_output = tf.keras.layers.LSTM(128,\n",
    "                                       return_sequences=True,\n",
    "                                       dropout=0.5)(element_wise_sum)\n",
    "    dropout = tf.keras.layers.Dropout(0.5, noise_shape=(None, 1, 128))(lstm_output)\n",
    "    dense = tf.keras.layers.Dense(embedding_dims + 3)(dropout)\n",
    "    relu = tf.keras.layers.ReLU()(dense)\n",
    "    un_embed = tf.keras.layers.Lambda(lambda x: tf.linalg.matmul(x, tf.transpose(embed_matrix)), num_tokens)(relu)\n",
    "    softmax = tf.keras.layers.Softmax()(un_embed)\n",
    "\n",
    "    model = tf.keras.models.Model([sequence_input, labels_input], softmax)\n",
    "    return model\n",
    "# model architecture inspired by https://arxiv.org/pdf/1511.05284.pdf, https://arxiv.org/pdf/1606.07770.pdf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compile_language_only_model(language_model):\n",
    "    def loss(labels, probabilities):\n",
    "        return tf.keras.losses.sparse_categorical_crossentropy(labels, probabilities)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "    language_model.compile(loss=loss, optimizer=optimizer)\n",
    "    return language_model\n",
    "\n",
    "\n",
    "def train_language_model(language_model, dataset, embedding_matrix, num_tokens, max_sequence_length, epochs):\n",
    "    embed_input = tf.matmul(tf.zeros((1, max_sequence_length - 1, num_tokens)),\n",
    "                            tf.convert_to_tensor(embedding_matrix, dtype=tf.float32))\n",
    "    for epoch in range(epochs):\n",
    "        for before, after in dataset:\n",
    "            language_model.train_on_batch([before, embed_input], after)\n",
    "\n",
    "\n",
    "def img_caption_func(img_name, caption):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    front, back = caption[:-1], caption[1:]\n",
    "    return img_tensor, front, back\n",
    "\n",
    "\n",
    "def create_image_captioning_model(img_classifier, language_model, embedding_matrix, embedding_dimensions):\n",
    "    image = img_classifier.input\n",
    "    image_labels = img_classifier.output\n",
    "    image_labels_embedded = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.matmul(x, tf.convert_to_tensor(embedding_matrix, dtype=tf.float32)),\n",
    "        embedding_dimensions + 3)(image_labels)\n",
    "    image_labels_embedded_tiled = tf.expand_dims(image_labels_embedded, axis=1)\n",
    "    sequence_input = language_model.input[0]\n",
    "    caption_output = language_model([sequence_input, image_labels_embedded_tiled])\n",
    "\n",
    "    model = tf.keras.models.Model([image, sequence_input], caption_output)\n",
    "    def loss(labels, probabilities):\n",
    "        return tf.keras.losses.sparse_categorical_crossentropy(labels, probabilities)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model\n",
    "# model architecture inspired by https://arxiv.org/pdf/1511.05284.pdf, https://arxiv.org/pdf/1606.07770.pdf\n",
    "\n",
    "\n",
    "\n",
    "def train_image_captioning_model(image_captioning_model, dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for image, front, back, in dataset:\n",
    "            image_captioning_model.train_on_batch([image, front], back)\n",
    "   \n",
    "\n",
    "def dataset_mapping_function(classifier_img_id, classifier_label, caption_img_id, caption_sequence, language_sequence):\n",
    "    classifier_img = np.load(classifier_img_id.decode('utf-8')+'.npy') \n",
    "    caption_img = np.load(caption_img_id.decode('utf-8')+'.npy') \n",
    "    label_one_hot = labels_to_one_hot(classifier_label, num_tokens)\n",
    "    caption_front, caption_back = caption_sequence[:-1], caption_sequence[1:]\n",
    "    language_front, language_back = language_sequence[:-1], language_sequence[1:]\n",
    "    return classifier_img, label_one_hot, caption_img, caption_front, caption_back, language_front, language_back\n",
    "\n",
    "\n",
    "def graph_bleu_curves(valid_bleu, car_mentions, classifier_loss, caption_loss, language_loss, iterations):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.plot(iterations, valid_bleu, label=\"Validation BLEU\")\n",
    "    plt.title(\"Validation BLEU Curves\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"BLEU Score\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"BLEU Curves.png\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.plot(iterations, car_mentions, label=\"Car(s) mentions in Captions\")\n",
    "    plt.title(\"Percentage of captions that correctly mention a car/cars\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"Car Curves.png\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.plot(iterations[1:], classifier_loss, label=\"Classifier Cross-Entropy\")\n",
    "    plt.plot(iterations[1:], classifier_loss, label=\"Language Model Cross-Entropy\")\n",
    "    plt.plot(iterations[1:], classifier_loss, label=\"Caption Model Cross-Entropy\")\n",
    "    plt.title(\"Cross-Entropy Curves\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cross-Entropy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"Cross-Entropy Curves.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def evaluate_caption_model(spiciness, image_captioning_model, valid_img_ids, valid_captions, max_sequence_length):\n",
    "    valid_imgs = []\n",
    "    for img_id in valid_img_ids:\n",
    "        valid_imgs.append(np.array(np.load(img_id+'.npy')))\n",
    "    valid_imgs = np.array(valid_imgs)\n",
    "    indices = np.ones((len(valid_imgs), 1), dtype=np.int32) * tokenizer.word_index[\"<start>\"]    \n",
    "    for i in range(1, max_sequence_length):\n",
    "        next_indices = indices[:,-1:]\n",
    "        predicted = image_captioning_model([valid_imgs, next_indices])/spiciness\n",
    "        # text generation partially adapted from https://www.tensorflow.org/tutorials/text/text_generation\n",
    "        predicted_ids = []\n",
    "        for prediction in predicted:\n",
    "            predicted_id = tf.random.categorical(prediction, num_samples=1)[-1,0].numpy()\n",
    "            predicted_ids.append([predicted_id])\n",
    "        indices = np.hstack((indices, np.array(predicted_ids)))\n",
    "    tokens = [[tokenizer.index_word[id] for id in row \n",
    "               if id != tokenizer.word_index[\"<pad>\"]] \n",
    "              for row in indices]\n",
    "    num_car_in_caption = [1 if \"car\" in set(sentence) or \"cars\" in set(sentence) else 0 for sentence in tokens]\n",
    "    average_score = 0\n",
    "    for i in range(len(tokens)):\n",
    "        average_score += nltk.translate.bleu_score.sentence_bleu(valid_captions[i], tokens[i], weights=(1, 0, 0, 0))\n",
    "    return average_score/len(tokens), sum(num_car_in_caption)/len(tokens)\n",
    "\n",
    "\n",
    "def simultaneous_train(epochs, image_classifier_model, language_model, image_captioning_model, dataset, embedding_matrix, num_tokens, max_sequence_length, valid_img_ids, valid_captions):\n",
    "    i = 0\n",
    "    validation_bleu = [0]\n",
    "    car_mentions = [0]\n",
    "    classifier_losses = []\n",
    "    language_model_losses = []\n",
    "    caption_model_losses = []\n",
    "    iterations = [0]\n",
    "    valid_img_ids = np.array(valid_img_ids)\n",
    "    valid_captions = np.array(valid_captions)\n",
    "    for epoch in range(epochs):\n",
    "        for classifier_img, label_one_hot, caption_img, caption_front, caption_back, language_front, language_back in dataset:\n",
    "            i += 1\n",
    "            classifier_loss = image_classifier_model.train_on_batch(classifier_img, label_one_hot)\n",
    "            embed_input = tf.matmul(tf.zeros((1, max_sequence_length - 1, num_tokens)),\n",
    "                                    tf.convert_to_tensor(embedding_matrix, dtype=tf.float32))\n",
    "            language_model_loss = language_model.train_on_batch([language_front, embed_input], language_back)\n",
    "            caption_loss = image_captioning_model.train_on_batch([caption_img, caption_front], caption_back)\n",
    "                        \n",
    "            manager.save()\n",
    "            \n",
    "            classifier_losses.append(classifier_loss)\n",
    "            language_model_losses.append(language_model_loss)\n",
    "            caption_model_losses.append(caption_loss)\n",
    "            \n",
    "            random_valid_indices = np.random.randint(0, len(valid_img_ids), 128)\n",
    "            bleu_score, percent_car_mentions = evaluate_caption_model(0.001, image_captioning_model, valid_img_ids[random_valid_indices], valid_captions[random_valid_indices], max_sequence_length)\n",
    "            validation_bleu.append(bleu_score)\n",
    "            car_mentions.append(percent_car_mentions)\n",
    "            iterations.append(i)\n",
    "            graph_bleu_curves(validation_bleu, car_mentions, classifier_losses, caption_model_losses, language_model_losses, iterations)\n",
    "\n",
    "\n",
    "\n",
    "train_img_ids, train_captions = img_ids_and_captions_from_json(PATH)\n",
    "# 27548 unique tokens in captions, 22128 present in glove embedding vocabulary\n",
    "\n",
    "# cache_vgg_features(train_img_ids)  # takes around 26 minutes with a GTX 1080 Ti to cache all images!!!\n",
    "raw_embedding_dims = 50\n",
    "glove_index = get_glove_embeddings(raw_embedding_dims)\n",
    "# save_preprocessed_captions(train_captions, glove_index)  # takes a couple minutes to process\n",
    "train_captions = retrieve_tokenized_captions_from_file()\n",
    "\n",
    "top_n_words = 2500\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_n_words, lower=True, oov_token=\"<unk>\",\n",
    "                                                  filters='\\t\\n')\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "tokenizer.word_index[\"<pad>\"] = 0  # index 1 is for oov tokens, 0 is for the <pad> token\n",
    "tokenizer.index_word[0] = \"<pad>\"\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_captions)\n",
    "max_sequence_length = max(len(sequence) for sequence in train_sequences)\n",
    "capped_sequences = tf.keras.preprocessing.sequence.pad_sequences(train_sequences,\n",
    "                                                                 maxlen=max_sequence_length,\n",
    "                                                                 padding=\"post\")\n",
    "# token preprocessing partially adapted from https://www.tensorflow.org/tutorials/text/image_captioning\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\") + list(\"!\\\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ '\") + [\"\\'s\",\n",
    "                                                                                                  \"<no_glove>\",\n",
    "                                                                                                  \"<start>\",\n",
    "                                                                                                  \"<end>\",\n",
    "                                                                                                  \"<unk>\",\n",
    "                                                                                                  \"<pad>\"]\n",
    "stop_word_indices = set(tokenizer.word_index.get(stop_word) for stop_word in stop_words\n",
    "                        if tokenizer.word_index.get(stop_word) is not None)\n",
    "\n",
    "# save_class_labels_from_capped_sequences(capped_sequences, stop_word_indices)  # takes ~80 seconds to process\n",
    "train_labels = retrieve_class_labels_from_capped_sequences()\n",
    "num_tokens = top_n_words\n",
    "max_num_labels_per_image = max(len(labels) for labels in train_labels)\n",
    "\n",
    "train_padded_labels = tf.keras.preprocessing.sequence.pad_sequences(train_labels, maxlen=max_num_labels_per_image + 10,\n",
    "                                                              padding=\"post\", value=-1)\n",
    "\n",
    "glove_matrix = get_glove_embedding_matrix(glove_index, tokenizer, num_tokens, raw_embedding_dims)\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 1000\n",
    "# 591753 images, 118287 are unique\n",
    "# image_classifier_dataset = tf.data.Dataset.from_tensor_slices((train_img_ids, train_padded_labels))\n",
    "# image_classifier_dataset = image_classifier_dataset.map(lambda item1, item2:\n",
    "#                                                         tf.numpy_function(img_classifier_func,\n",
    "#                                                                           [item1, item2],\n",
    "#                                                                           [tf.float32, tf.float32]),\n",
    "#                                                         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# image_classifier_dataset = image_classifier_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "# image_classifier_dataset = image_classifier_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "partial_img_classifier = create_partial_image_classifier(num_tokens)\n",
    "img_classifier = compile_classifier_only_model(partial_img_classifier)\n",
    "# train_image_classifier(img_classifier, image_classifier_dataset, 1)\n",
    "\n",
    "partial_language_model = create_partial_lstm_language_model(num_tokens, max_sequence_length, raw_embedding_dims, glove_matrix)\n",
    "language_model = compile_language_only_model(partial_language_model)\n",
    "\n",
    "# BATCH_SIZE = 256\n",
    "# langauge_model_dataset = tf.data.Dataset.from_tensor_slices(capped_sequences)\n",
    "# langauge_model_dataset = langauge_model_dataset.map(lambda sequence: (sequence[:-1], sequence[1:]))\n",
    "# langauge_model_dataset = langauge_model_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "# langauge_model_dataset = langauge_model_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "# train_language_model(language_model, langauge_model_dataset, glove_matrix, num_tokens, max_sequence_length, 1)\n",
    "\n",
    "\n",
    "# img_caption_dataset = tf.data.Dataset.from_tensor_slices((train_img_ids, capped_sequences))\n",
    "# img_caption_dataset = img_caption_dataset.map(lambda item1, item2:\n",
    "#                                               tf.numpy_function(img_caption_func,\n",
    "#                                                                 [item1, item2],\n",
    "#                                                                 [tf.float32, tf.int32, tf.int32]),\n",
    "#                                               num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# img_caption_dataset = img_caption_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "# img_caption_dataset = img_caption_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "image_captioning_model = create_image_captioning_model(partial_img_classifier, partial_language_model,\n",
    "                                                       glove_matrix, raw_embedding_dims)\n",
    "# train_image_captioning_model(image_captioning_model, img_caption_dataset, 1)\n",
    "\n",
    "caption_img_ids, caption_sequences = list(zip(*[(id, caption) for id, caption in zip(train_img_ids, capped_sequences) \n",
    "                                                 if tokenizer.word_index[\"car\"] not in set(caption) \n",
    "                                                 and tokenizer.word_index[\"cars\"] not in set(caption)]))\n",
    "caption_img_ids, caption_sequences = list(caption_img_ids), list(caption_sequences)\n",
    "\n",
    "classifier_img_ids = train_img_ids.copy()\n",
    "language_model_sequences = capped_sequences.copy()\n",
    "num_subset_rows = len(caption_img_ids)\n",
    "\n",
    "_ = list(zip(classifier_img_ids, train_padded_labels))\n",
    "np.random.shuffle(_)\n",
    "classifier_img_ids, classifier_labels = list(zip(*_[:num_subset_rows]))\n",
    "classifier_img_ids, classifier_labels = list(classifier_img_ids), list(classifier_labels)\n",
    "np.random.shuffle(language_model_sequences)\n",
    "language_model_sequences = language_model_sequences[:num_subset_rows]\n",
    "\n",
    "valid_img_ids, valid_sequences = list(zip(*[(id, caption) for id, caption in zip(train_img_ids, capped_sequences) \n",
    "                                                 if tokenizer.word_index[\"car\"] in set(caption) \n",
    "                                                 or tokenizer.word_index[\"cars\"] in set(caption)]))\n",
    "valid_img_ids, valid_sequences = list(valid_img_ids), list(valid_sequences)\n",
    "valid_captions = [[tokenizer.index_word[index] for index in caption if index != tokenizer.word_index[\"<pad>\"]] for caption in valid_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_way_dataset = tf.data.Dataset.from_tensor_slices((classifier_img_ids, \n",
    "                                                        classifier_labels,\n",
    "                                                        caption_img_ids, \n",
    "                                                        caption_sequences,\n",
    "                                                        language_model_sequences))\n",
    "three_way_dataset = three_way_dataset.map(lambda item1, item2, item3, item4, item5:\n",
    "                                                        tf.numpy_function(dataset_mapping_function,\n",
    "                                                                          [item1, item2, item3, item4, item5],\n",
    "                                                                          [tf.float32, tf.float32, tf.float32, tf.int32, tf.int32, tf.int32, tf.int32]),\n",
    "                                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# dataset API tutorial provided by https://www.tensorflow.org/tutorials/text/image_captioning\n",
    "three_way_dataset = three_way_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "three_way_dataset = three_way_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc893f197f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(img_classifier = img_classifier, \n",
    "                                 language_model = language_model, \n",
    "                                 image_captioning_model = image_captioning_model)\n",
    "\n",
    "#checkpointing tutorial provided by https://www.tensorflow.org/tutorials/generative/dcgan\n",
    "manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_dir, max_to_keep=1)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simultaneous_train(10, img_classifier, language_model, image_captioning_model, three_way_dataset, glove_matrix, num_tokens, max_sequence_length, valid_img_ids, valid_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions(spiciness, image_captioning_model, valid_img_ids, valid_captions, max_sequence_length):\n",
    "    valid_imgs = []\n",
    "    for img_id in valid_img_ids:\n",
    "        valid_imgs.append(np.array(np.load(img_id+'.npy')))\n",
    "    valid_imgs = np.array(valid_imgs)\n",
    "    indices = np.ones((len(valid_imgs), 1), dtype=np.int32) * tokenizer.word_index[\"<start>\"]    \n",
    "    for i in range(1, max_sequence_length):\n",
    "        predicted = image_captioning_model([valid_imgs, indices])/spiciness\n",
    "        # text generation partially adapted from https://www.tensorflow.org/tutorials/text/text_generation\n",
    "        predicted_ids = []\n",
    "        for prediction in predicted:\n",
    "            predicted_id = tf.random.categorical(prediction, num_samples=1)[-1,0].numpy()\n",
    "            predicted_ids.append([predicted_id])\n",
    "        indices = np.hstack((indices, np.array(predicted_ids)))\n",
    "    tokens = [[tokenizer.index_word[id] for id in row \n",
    "               if id != tokenizer.word_index[\"<pad>\"]] \n",
    "              for row in indices]\n",
    "    non_car_captions = [a for a in list(zip(valid_img_ids,valid_captions,tokens)) if \"car\" not in set(a[2]) and \"cars\" not in set(a[2])]\n",
    "    car_captions = [a for a in list(zip(valid_img_ids,valid_captions,tokens)) if \"car\" in set(a[2]) or \"cars\" in set(a[2])]\n",
    "    i = 1\n",
    "    for caption in non_car_captions:\n",
    "        if i > 25:\n",
    "            break\n",
    "        fig = plt.figure(figsize=(8, 8)).Embedding(input_dim=num_tokens,\n",
    "                                                   output_dim=embedding_dims + 3,\n",
    "                                                   embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                                                   trainable=False)(sequence_input)\n",
    "\n",
    "    labels_input = tf.keras.layers.Input(shape=(None, embedding_dims + 3, ))\n",
    "\n",
    "\n",
    "    element_wise_sum = tf.keras.layers.Add()([embedded_sequences, labels_input])\n",
    "\n",
    "    lstm_output = tf.keras.layers.LSTM(128,\n",
    "                                       return_sequences=True,\n",
    "                                       dropout=0.5)(element_wise_sum)\n",
    "    dropout = tf.keras.layers.Dropout(0.5, noise_shape=(None, 1, 128))(lstm_output)\n",
    "    dense = tf.keras.layers.Dense(embedding_dims + 3)(dropout)\n",
    "    relu = tf.keras.layers.ReLU()(dense)\n",
    "    un_embed = tf.keras.layers.Lambda(lamb\n",
    "        img = plt.imread(caption[0])\n",
    "        plt.imshow(img)\n",
    "        plt.title(\" \".join(caption[1]) +\"\\n\" + \" \".join(caption[2]))\n",
    "        plt.axis('off')\n",
    "        plt.savefig(str(i)+\".jpg\")\n",
    "        plt.close(fig)\n",
    "        i += 1\n",
    "        \n",
    "    i = 100\n",
    "    for caption in car_captions:\n",
    "        if i > 125:\n",
    "            break\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        img = plt.imread(caption[0])\n",
    "        plt.imshow(img)\n",
    "        plt.title(\" \".join(caption[1]) +\"\\n\" + \" \".join(caption[2]))\n",
    "        plt.axis('off')\n",
    "        plt.savefig(str(i)+\".jpg\")\n",
    "        plt.close(fig)\n",
    "        i += 1\n",
    "        \n",
    "generate_captions(0.001, image_captioning_model, valid_img_ids[:1024], valid_captions[:1024], max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import nltk\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import decode_predictions, preprocess_input\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "assert tf.__version__ == \"2.0.0\"\n",
    "\n",
    "PATH = \"/media/jintoboy/Main Storage/Image Captioning/\"\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    if height != width:\n",
    "        if height < width:\n",
    "            offset = int((width - height) / 2)\n",
    "            img = tf.image.crop_to_bounding_box(img, 0, offset, height, height)\n",
    "        else:\n",
    "            offset = int((height - width) / 2)\n",
    "            img = tf.image.crop_to_bounding_box(img, offset, 0, width, width)\n",
    "    img = tf.image.resize(img, (224, 224))\n",
    "    return preprocess_input(img), image_path\n",
    "\n",
    "\n",
    "def img_ids_and_captions_from_json(PATH):\n",
    "    with open(PATH + \"annotations/captions_train2017.json\", \"r\") as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    img_locations = []\n",
    "    captions = []\n",
    "    for annotation in annotations[\"annotations\"]:\n",
    "        img_locations.append(\"{}train2017/{:012d}.jpg\".format(PATH, annotation[\"image_id\"]))\n",
    "        captions.append(annotation[\"caption\"])\n",
    "\n",
    "    return img_locations, captions\n",
    "    # partially adapted from https://www.tensorflow.org/tutorials/text/image_captioning\n",
    "\n",
    "\n",
    "def cache_vgg_features(img_ids):\n",
    "    unique_ids = sorted(set(img_ids))\n",
    "    images = tf.data.Dataset.from_tensor_slices(unique_ids)\n",
    "    images = images.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "    extract_vgg_features_model = VGG16(include_top=False)\n",
    "\n",
    "    for img, path in tqdm(images):\n",
    "        batch_features = extract_vgg_features_model(img)\n",
    "        batch_features = tf.reshape(batch_features, (batch_features.shape[0],\n",
    "                                                     -1, batch_features.shape[3]))\n",
    "        for bf, p in zip(batch_features, path):\n",
    "            path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "            np.save(path_of_feature, bf.numpy())\n",
    "    # adapted from https://www.tensorflow.org/tutorials/text/image_captioning\n",
    "\n",
    "\n",
    "def get_glove_embeddings(dimensions):\n",
    "    # 400000 unique tokens in vocabulary\n",
    "    embeddings_index = {}\n",
    "    with open(os.path.join(\"{}glove_embeddings\".format(PATH), \"glove.6B.{}d.txt\".format(dimensions))) as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs + \" 0 0 0\", 'f', sep=' ')  # add 3 dimensions for <start>, <end>, and <pad>\n",
    "            embeddings_index[word] = coefs\n",
    "    # partially adapted from https://keras.io/examples/pretrained_word_embeddings/\n",
    "    \n",
    "    unk_vector = np.zeros((dimensions + 3,))\n",
    "    i = 0\n",
    "    for word in embeddings_index:\n",
    "        unk_vector += embeddings_index[word]\n",
    "        i += 1\n",
    "    unk_vector /= i\n",
    "    embeddings_index[\"<unk>\"] = unk_vector\n",
    "    embeddings_index[\"<no_glove>\"] = unk_vector\n",
    "\n",
    "    embeddings_index[\"<start>\"] = np.zeros((dimensions + 3,))\n",
    "    embeddings_index[\"<start>\"][dimensions] = 1\n",
    "\n",
    "    embeddings_index[\"<end>\"] = np.zeros((dimensions + 3,))\n",
    "    embeddings_index[\"<end>\"][dimensions + 1] = 1\n",
    "\n",
    "    embeddings_index[\"<pad>\"] = np.zeros((dimensions + 3,))\n",
    "    embeddings_index[\"<pad>\"][dimensions + 2] = 1\n",
    "\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def get_glove_embedding_matrix(processed_glove_indices, tokenizer, num_words, raw_embedding_dims):\n",
    "    embedding_matrix = np.zeros((num_words, raw_embedding_dims + 3))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i >= num_words:\n",
    "            continue\n",
    "        embedding_matrix[i] = processed_glove_indices.get(word)\n",
    "    # partially adapted from https://keras.io/examples/pretrained_word_embeddings/\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def save_preprocessed_captions(captions, glove_embedding_index):\n",
    "    glove_words = set(glove_embedding_index.keys())\n",
    "    for i in range(len(captions)):\n",
    "        tokens = [token.lower() if token.lower() in glove_words\n",
    "                  else \"<no_glove>\"\n",
    "                  for token in nltk.tokenize.word_tokenize(captions[i])]\n",
    "        captions[i] = \"<start> {} <end>\".format(\" \".join(tokens))\n",
    "    captions_file = open(\"{}annotations/tokenized_captions.txt\".format(PATH), \"w\")\n",
    "    for caption in captions:\n",
    "        print(caption, file=captions_file)\n",
    "    captions_file.close()\n",
    "\n",
    "\n",
    "def retrieve_tokenized_captions_from_file():\n",
    "    file = open(\"{}annotations/tokenized_captions.txt\".format(PATH), \"r\")\n",
    "    lines = [line.rstrip('\\n') for line in file]\n",
    "    file.close()\n",
    "    return lines\n",
    "\n",
    "\n",
    "def save_class_labels_from_capped_sequences(capped_seqs, stop_word_tokenizer_indices):\n",
    "    train_classes = [list(set([token for token in sequence if token not in stop_word_tokenizer_indices]))\n",
    "                     for sequence in capped_seqs]\n",
    "    labels_file = open(\"{}annotations/class_labels.txt\".format(PATH), \"w\")\n",
    "    for labels in train_classes:\n",
    "        print(\" \".join(map(str, labels)), file=labels_file)\n",
    "    labels_file.close()\n",
    "\n",
    "\n",
    "def retrieve_class_labels_from_capped_sequences():\n",
    "    file = open(\"{}annotations/class_labels.txt\".format(PATH), \"r\")\n",
    "    lines = [np.fromstring(line.rstrip('\\n'), int, sep=' ') for line in file]\n",
    "    file.close()\n",
    "    return lines\n",
    "\n",
    "\n",
    "def labels_to_one_hot(labels, num_labels):\n",
    "    return tf.reduce_sum(tf.one_hot(labels, depth=num_labels), axis=0)\n",
    "\n",
    "\n",
    "def one_hot_to_labels(one_hot):\n",
    "    where = tf.not_equal(one_hot, 0)\n",
    "    indices = tf.where(where)\n",
    "    value_rowids = indices[:, 0]\n",
    "    values = indices[:, 1]\n",
    "    ragged_tensor = tf.RaggedTensor.from_value_rowids(values, value_rowids)\n",
    "    return [tensor.numpy() for tensor in list(ragged_tensor)]\n",
    "\n",
    "\n",
    "def img_classifier_func(img_name, labels):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    one_hot = labels_to_one_hot(labels, num_tokens)\n",
    "    return img_tensor, one_hot\n",
    "\n",
    "\n",
    "def create_partial_image_classifier(num_tokens):\n",
    "# VGG output is 7x7x512, but the cached files are 49 x 512\n",
    "    vgg_input = tf.keras.layers.Input(shape=(49, 512))\n",
    "    flattened = tf.keras.layers.Flatten()(vgg_input)\n",
    "    dropout = tf.keras.layers.Dropout(0.5)(flattened)\n",
    "    dense_output = tf.keras.layers.Dense(num_tokens, activation=\"sigmoid\")(dropout)\n",
    "\n",
    "    model = tf.keras.models.Model([vgg_input], dense_output)\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_classifier_only_model(classifier_model):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "    classifier_model.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n",
    "    return classifier_model\n",
    "\n",
    "\n",
    "def train_image_classifier(image_classifier_model, dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for imgs, labels in dataset:\n",
    "            image_classifier_model.train_on_batch([imgs], labels)\n",
    "\n",
    "\n",
    "def create_partial_lstm_language_model(num_tokens, max_sequence_length, embedding_dims, embedding_matrix):\n",
    "    embed_matrix = tf.convert_to_tensor(embedding_matrix, dtype=tf.float32)\n",
    "\n",
    "    sequence_input = tf.keras.layers.Input(shape=(None,))\n",
    "    embedded_sequences = tf.keras.layers.Embedding(input_dim=num_tokens,\n",
    "                                                   output_dim=embedding_dims + 3,\n",
    "                                                   embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "                                                   trainable=False)(sequence_input)\n",
    "\n",
    "    labels_input = tf.keras.layers.Input(shape=(None, embedding_dims + 3, ))\n",
    "\n",
    "\n",
    "    element_wise_sum = tf.keras.layers.Add()([embedded_sequences, labels_input])\n",
    "\n",
    "    lstm_output = tf.keras.layers.LSTM(128,\n",
    "                                       return_sequences=True,\n",
    "                                       dropout=0.5)(element_wise_sum)\n",
    "    dropout = tf.keras.layers.Dropout(0.5, noise_shape=(None, 1, 128))(lstm_output)\n",
    "    dense = tf.keras.layers.Dense(embedding_dims + 3)(dropout)\n",
    "    relu = tf.keras.layers.ReLU()(dense)\n",
    "    un_embed = tf.keras.layers.Lambda(lambda x: tf.linalg.matmul(x, tf.transpose(embed_matrix)), num_tokens)(relu)\n",
    "    softmax = tf.keras.layers.Softmax()(un_embed)\n",
    "\n",
    "    model = tf.keras.models.Model([sequence_input, labels_input], softmax)\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_language_only_model(language_model):\n",
    "    def loss(labels, probabilities):\n",
    "        return tf.keras.losses.sparse_categorical_crossentropy(labels, probabilities)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "    language_model.compile(loss=loss, optimizer=optimizer)\n",
    "    return language_model\n",
    "\n",
    "\n",
    "def train_language_model(language_model, dataset, embedding_matrix, num_tokens, max_sequence_length, epochs):\n",
    "    embed_input = tf.matmul(tf.zeros((1, max_sequence_length - 1, num_tokens)),\n",
    "                            tf.convert_to_tensor(embedding_matrix, dtype=tf.float32))\n",
    "    for epoch in range(epochs):\n",
    "        for before, after in dataset:\n",
    "            language_model.train_on_batch([before, embed_input], after)\n",
    "\n",
    "\n",
    "def img_caption_func(img_name, caption):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    front, back = caption[:-1], caption[1:]\n",
    "    return img_tensor, front, back\n",
    "\n",
    "\n",
    "def create_image_captioning_model(img_classifier, language_model, embedding_matrix, embedding_dimensions):\n",
    "    image = img_classifier.input\n",
    "    image_labels = img_classifier.output\n",
    "    image_labels_embedded = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.matmul(x, tf.convert_to_tensor(embedding_matrix, dtype=tf.float32)),\n",
    "        embedding_dimensions + 3)(image_labels)\n",
    "    image_labels_embedded_tiled = tf.expand_dims(image_labels_embedded, axis=1)\n",
    "    sequence_input = language_model.input[0]\n",
    "    caption_output = language_model([sequence_input, image_labels_embedded_tiled])\n",
    "\n",
    "    model = tf.keras.models.Model([image, sequence_input], caption_output)\n",
    "    def loss(labels, probabilities):\n",
    "        return tf.keras.losses.sparse_categorical_crossentropy(labels, probabilities)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_image_captioning_model(image_captioning_model, dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for image, front, back, in dataset:\n",
    "            image_captioning_model.train_on_batch([image, front], back)\n",
    "   \n",
    "\n",
    "def dataset_mapping_function(classifier_img_id, classifier_label, caption_img_id, caption_sequence, language_sequence):\n",
    "    classifier_img = np.load(classifier_img_id.decode('utf-8')+'.npy') \n",
    "    caption_img = np.load(caption_img_id.decode('utf-8')+'.npy') \n",
    "    label_one_hot = labels_to_one_hot(classifier_label, num_tokens)\n",
    "    caption_front, caption_back = caption_sequence[:-1], caption_sequence[1:]\n",
    "    language_front, language_back = language_sequence[:-1], language_sequence[1:]\n",
    "    return classifier_img, label_one_hot, caption_img, caption_front, caption_back, language_front, language_back\n",
    "\n",
    "\n",
    "def graph_bleu_curves(valid_bleu, car_mentions, classifier_loss, caption_loss, language_loss, iterations):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.plot(iterations, valid_bleu, label=\"Validation BLEU\")\n",
    "    plt.title(\"Validation BLEU Curves\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"BLEU Score\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"BLEU Curves.png\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.plot(iterations, car_mentions, label=\"Car(s) mentions in Captions\")\n",
    "    plt.title(\"Percentage of captions that correctly mention a car/cars\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"Car Curves.png\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.plot(iterations[1:], classifier_loss, label=\"Classifier Cross-Entropy\")\n",
    "    plt.plot(iterations[1:], classifier_loss, label=\"Language Model Cross-Entropy\")\n",
    "    plt.plot(iterations[1:], classifier_loss, label=\"Caption Model Cross-Entropy\")\n",
    "    plt.title(\"Cross-Entropy Curves\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cross-Entropy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"Cross-Entropy Curves.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def evaluate_caption_model(spiciness, image_captioning_model, valid_img_ids, valid_captions, max_sequence_length):\n",
    "    valid_imgs = []\n",
    "    for img_id in valid_img_ids:\n",
    "        valid_imgs.append(np.array(np.load(img_id+'.npy')))\n",
    "    valid_imgs = np.array(valid_imgs)\n",
    "    indices = np.ones((len(valid_imgs), 1), dtype=np.int32) * tokenizer.word_index[\"<start>\"]    \n",
    "    for i in range(1, max_sequence_length):\n",
    "        next_indices = indices[:,-1:]\n",
    "        predicted = image_captioning_model([valid_imgs, next_indices])/spiciness\n",
    "        # text generation partially adapted from https://www.tensorflow.org/tutorials/text/text_generation\n",
    "        predicted_ids = []\n",
    "        for prediction in predicted:\n",
    "            predicted_id = tf.random.categorical(prediction, num_samples=1)[-1,0].numpy()\n",
    "            predicted_ids.append([predicted_id])\n",
    "        indices = np.hstack((indices, np.array(predicted_ids)))\n",
    "    tokens = [[tokenizer.index_word[id] for id in row \n",
    "               if id != tokenizer.word_index[\"<pad>\"]] \n",
    "              for row in indices]\n",
    "    num_car_in_caption = [1 if \"car\" in set(sentence) or \"cars\" in set(sentence) else 0 for sentence in tokens]\n",
    "    average_score = 0\n",
    "    for i in range(len(tokens)):\n",
    "        average_score += nltk.translate.bleu_score.sentence_bleu(valid_captions[i], tokens[i], weights=(1, 0, 0, 0))\n",
    "    return average_score/len(tokens), sum(num_car_in_caption)/len(tokens)\n",
    "\n",
    "\n",
    "def simultaneous_train(epochs, image_classifier_model, language_model, image_captioning_model, dataset, embedding_matrix, num_tokens, max_sequence_length, valid_img_ids, valid_captions):\n",
    "    i = 0\n",
    "    validation_bleu = [0]\n",
    "    car_mentions = [0]\n",
    "    classifier_losses = []\n",
    "    language_model_losses = []\n",
    "    caption_model_losses = []\n",
    "    iterations = [0]\n",
    "    valid_img_ids = np.array(valid_img_ids)\n",
    "    valid_captions = np.array(valid_captions)\n",
    "    for epoch in range(epochs):\n",
    "        for classifier_img, label_one_hot, caption_img, caption_front, caption_back, language_front, language_back in dataset:\n",
    "            i += 1\n",
    "            classifier_loss = image_classifier_model.train_on_batch(classifier_img, label_one_hot)\n",
    "            embed_input = tf.matmul(tf.zeros((1, max_sequence_length - 1, num_tokens)),\n",
    "                                    tf.convert_to_tensor(embedding_matrix, dtype=tf.float32))\n",
    "            language_model_loss = language_model.train_on_batch([language_front, embed_input], language_back)\n",
    "            caption_loss = image_captioning_model.train_on_batch([caption_img, caption_front], caption_back)\n",
    "                        \n",
    "            manager.save()\n",
    "            \n",
    "            classifier_losses.append(classifier_loss)\n",
    "            language_model_losses.append(language_model_loss)\n",
    "            caption_model_losses.append(caption_loss)\n",
    "            \n",
    "            random_valid_indices = np.random.randint(0, len(valid_img_ids), 128)\n",
    "            bleu_score, percent_car_mentions = evaluate_caption_model(0.001, image_captioning_model, valid_img_ids[random_valid_indices], valid_captions[random_valid_indices], max_sequence_length)\n",
    "            validation_bleu.append(bleu_score)\n",
    "            car_mentions.append(percent_car_mentions)\n",
    "            iterations.append(i)\n",
    "            graph_bleu_curves(validation_bleu, car_mentions, classifier_losses, caption_model_losses, language_model_losses, iterations)\n",
    "\n",
    "\n",
    "\n",
    "train_img_ids, train_captions = img_ids_and_captions_from_json(PATH)\n",
    "# 27548 unique tokens in captions, 22128 present in glove embedding vocabulary\n",
    "\n",
    "# cache_vgg_features(train_img_ids)  # takes around 26 minutes with a GTX 1080 Ti to cache all images!!!\n",
    "raw_embedding_dims = 50\n",
    "glove_index = get_glove_embeddings(raw_embedding_dims)\n",
    "# save_preprocessed_captions(train_captions, glove_index)  # takes a couple minutes to process\n",
    "train_captions = retrieve_tokenized_captions_from_file()\n",
    "\n",
    "top_n_words = 2500\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_n_words, lower=True, oov_token=\"<unk>\",\n",
    "                                                  filters='\\t\\n')\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "tokenizer.word_index[\"<pad>\"] = 0  # index 1 is for oov tokens, 0 is for the <pad> token\n",
    "tokenizer.index_word[0] = \"<pad>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index[\"<no_glove>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_captions)\n",
    "max_sequence_length = max(len(sequence) for sequence in train_sequences)\n",
    "capped_sequences = tf.keras.preprocessing.sequence.pad_sequences(train_sequences,\n",
    "                                                                 maxlen=max_sequence_length,\n",
    "                                                                 padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_gloves = [i for i, sequence in enumerate(capped_sequences) if tokenizer.word_index[\"<no_glove>\"] in set(sequence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_ids, train_captions = img_ids_and_captions_from_json(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10536"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([train_captions[i] for i in no_gloves])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118287"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_img_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
